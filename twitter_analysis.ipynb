{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import emoji\n",
    "from operator import add\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "sentiment_dictionary = {}\n",
    "emoji_sentiment_dictionary = {}\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "\n",
    "# load negative/positive words to python lists for tweet analysis\n",
    "\n",
    "with open('positive_words.txt') as f:\n",
    "    positive_words = f.read().splitlines()\n",
    "    \n",
    "with open('negative_words.txt') as f:\n",
    "    negative_words = f.read().splitlines()\n",
    "    \n",
    "# load emoji sentiment\n",
    "with open('emoji_sentiment.json') as j:\n",
    "    emoji_sentiment_dictionary = json.load(j)\n",
    "    \n",
    "# with open('dict.tff','r') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "translate = {\n",
    "    \"weak\":0.5,\n",
    "    \"strong\":1,\n",
    "    \"positive\":1,\n",
    "    \"neutral\":0,\n",
    "    \"both\":0,\n",
    "    \"negative\":-1\n",
    "}\n",
    "\n",
    "# for line in lines:\n",
    "#     word = re.sub(r'.*word1=([a-z]+)\\spos.*\\n', r'\\1', line)\n",
    "#     subjectivity = re.sub(r'.*type=([a-z]+)subj.*\\n', r'\\1', line)\n",
    "#     polarity = re.sub(r'.*priorpolarity=([a-z]+)\\n', r'\\1', line)\n",
    "#     sentiment_dictionary[word] = translate[polarity] * translate[subjectivity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    \"i\",\n",
    "    \"me\",\n",
    "    \"my\",\n",
    "    \"myself\",\n",
    "    \"we\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"ourselves\",\n",
    "    \"you\",\n",
    "    \"your\",\n",
    "    \"yours\",\n",
    "    \"yourself\",\n",
    "    \"yourselves\",\n",
    "    \"he\",\n",
    "    \"him\",\n",
    "    \"his\",\n",
    "    \"himself\",\n",
    "    \"she\",\n",
    "    \"her\",\n",
    "    \"hers\",\n",
    "    \"herself\",\n",
    "    \"it\",\n",
    "    \"its\",\n",
    "    \"itself\",\n",
    "    \"they\",\n",
    "    \"them\",\n",
    "    \"their\",\n",
    "    \"theirs\",\n",
    "    \"themselves\",\n",
    "    \"what\",\n",
    "    \"which\",\n",
    "    \"who\",\n",
    "    \"whom\",\n",
    "    \"this\",\n",
    "    \"that\",\n",
    "    \"these\",\n",
    "    \"those\",\n",
    "    \"am\",\n",
    "    \"is\",\n",
    "    \"are\",\n",
    "    \"was\",\n",
    "    \"were\",\n",
    "    \"be\",\n",
    "    \"been\",\n",
    "    \"being\",\n",
    "    \"have\",\n",
    "    \"has\",\n",
    "    \"had\",\n",
    "    \"having\",\n",
    "    \"do\",\n",
    "    \"does\",\n",
    "    \"did\",\n",
    "    \"doing\",\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    \"and\",\n",
    "    \"but\",\n",
    "    \"if\",\n",
    "    \"or\",\n",
    "    \"because\",\n",
    "    \"as\",\n",
    "    \"until\",\n",
    "    \"while\",\n",
    "    \"of\",\n",
    "    \"at\",\n",
    "    \"by\",\n",
    "    \"for\",\n",
    "    \"with\",\n",
    "    \"about\",\n",
    "    \"against\",\n",
    "    \"between\",\n",
    "    \"into\",\n",
    "    \"through\",\n",
    "    \"during\",\n",
    "    \"before\",\n",
    "    \"after\",\n",
    "    \"above\",\n",
    "    \"below\",\n",
    "    \"to\",\n",
    "    \"from\",\n",
    "    \"up\",\n",
    "    \"down\",\n",
    "    \"in\",\n",
    "    \"out\",\n",
    "    \"on\",\n",
    "    \"off\",\n",
    "    \"over\",\n",
    "    \"under\",\n",
    "    \"again\",\n",
    "    \"further\",\n",
    "    \"then\",\n",
    "    \"once\",\n",
    "    \"here\",\n",
    "    \"there\",\n",
    "    \"when\",\n",
    "    \"where\",\n",
    "    \"why\",\n",
    "    \"how\",\n",
    "    \"all\",\n",
    "    \"any\",\n",
    "    \"both\",\n",
    "    \"each\",\n",
    "    \"few\",\n",
    "    \"more\",\n",
    "    \"most\",\n",
    "    \"other\",\n",
    "    \"some\",\n",
    "    \"such\",\n",
    "    \"no\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"only\",\n",
    "    \"own\",\n",
    "    \"same\",\n",
    "    \"so\",\n",
    "    \"than\",\n",
    "    \"too\",\n",
    "    \"very\",\n",
    "    \"s\",\n",
    "    \"t\",\n",
    "    \"can\",\n",
    "    \"will\",\n",
    "    \"just\",\n",
    "    \"don\",\n",
    "    \"should\",\n",
    "    \"now\"\n",
    "]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    to_remove = '|'.join(stopwords)\n",
    "    regex = re.compile(r'\\b('+to_remove+r')\\b', flags=re.IGNORECASE)\n",
    "    return regex.sub(\"\", text)\n",
    "\n",
    "def is_url(text):\n",
    "    if re.search(r'(https?:\\/\\/[^\\s]+)', text):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def lemmatize(text):\n",
    "    return [lemmatizer.lemmatize(token.lower()) for token in text]\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join([word for word in text.split() if not is_url(word)])\n",
    "    return re.findall(r'[a-zA-Z0-9-\\']+', text)\n",
    "\n",
    "# function returning unique emojis in a string\n",
    "def get_emojis(text):\n",
    "    return list(set([fix_emoji(c) for c in text if c in emoji.UNICODE_EMOJI]))\n",
    "\n",
    "def fix_emoji(emoji):\n",
    "    ret = re.sub(br\".*(\\\\[^\\\\]*)$\", br'\\1' ,emoji.encode('unicode-escape')).decode('unicode-escape')\n",
    "    return ret\n",
    "\n",
    "def evaluate_by_emojis(text):\n",
    "    negative = 0\n",
    "    positive = 0\n",
    "    for emoji in get_emojis(text):\n",
    "        if emoji in emoji_sentiment_dictionary:\n",
    "            if emoji_sentiment_dictionary[emoji][\"positive-emotion\"] > emoji_sentiment_dictionary[emoji][\"negative-emotion\"]:\n",
    "                positive += 1\n",
    "            else:\n",
    "                negative += 1\n",
    "            \n",
    "    return 1 if positive>negative else -1\n",
    "\n",
    "def evaluate_by_words(text):\n",
    "    negative = 0\n",
    "    positive = 0\n",
    "    for token in tokenize(text):\n",
    "        token = token.lower()\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        if token in positive_words:\n",
    "            positive += 1\n",
    "        if token in negative_words:\n",
    "            negative += 1\n",
    "            \n",
    "    if positive == negative:\n",
    "        return 0\n",
    "    return 1 if positive>negative else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters for streaming pipeline\n",
    "\n",
    "def language_filter(text, lang):\n",
    "    try:\n",
    "        if detect(text) == lang:\n",
    "            return True\n",
    "    except:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def no_retweet(text):\n",
    "    if text.startswith('RT @'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def split(tweet):\n",
    "    text = remove_stopwords(tweet['text'])\n",
    "    sentiment = tweet['sentiment']\n",
    "    return [\n",
    "        (lemmatizer.lemmatize(token.lower()), (sentiment, 1))\n",
    "        for token in tokenize(text)\n",
    "    ]\n",
    "\n",
    "def split_emojis(tweet):\n",
    "    emojis = get_emojis(tweet['text'])\n",
    "    sentiment = tweet['sentiment']\n",
    "    \n",
    "    return [\n",
    "        (emoji, (sentiment, 1))\n",
    "        for emoji in emojis\n",
    "    ]\n",
    "\n",
    "def only_emojis(text):\n",
    "    return True if len(get_emojis(text)) > 0 else False\n",
    "\n",
    "def add_tuples(a,b):\n",
    "    return a[0]+b[0], a[1]+b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"twitter_stream_2020_03_01/03/01/[0-1][0-9]\"\n",
    "spark = SparkSession.builder.appName(\"Sentiment App\").getOrCreate()\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "data = (\n",
    "    spark\n",
    "        .read\n",
    "        .json(inputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_words = (\n",
    "    data\n",
    "        .rdd\n",
    "        .filter(lambda tweet: tweet['text'] is not None)\n",
    "        .filter(lambda tweet: no_retweet(tweet['text']))\n",
    "        .filter(lambda tweet: only_emojis(tweet['text']))\n",
    "        .filter(lambda tweet: language_filter(tweet['text'], 'en'))\n",
    "        .map(lambda tweet: {\n",
    "            'sentiment':evaluate_by_emojis(tweet['text']), \n",
    "            'text':tweet['text']\n",
    "        })\n",
    "        .flatMap(lambda tweet: split(tweet))\n",
    "        .reduceByKey(add_tuples)\n",
    "        .map(lambda row: (row[0], row[1][0], row[1][1]))\n",
    ")\n",
    "\n",
    "df = evaluated_words.toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename stuff, create ratio column \n",
    "df['ratio']=df.apply(lambda row: row['_2'] / row['_3'], axis=1)\n",
    "df = df.rename(columns={'_1':'word', '_2':'score', '_3':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "      <th>count</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53253</th>\n",
       "      <td>maternal</td>\n",
       "      <td>-28</td>\n",
       "      <td>28</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27459</th>\n",
       "      <td>downshifter</td>\n",
       "      <td>-16</td>\n",
       "      <td>16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88127</th>\n",
       "      <td>paternal</td>\n",
       "      <td>-16</td>\n",
       "      <td>16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56772</th>\n",
       "      <td>aq7481-600</td>\n",
       "      <td>-14</td>\n",
       "      <td>14</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25976</th>\n",
       "      <td>5h</td>\n",
       "      <td>-13</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67446</th>\n",
       "      <td>sony</td>\n",
       "      <td>-9</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7341</th>\n",
       "      <td>recep</td>\n",
       "      <td>-7</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>esan</td>\n",
       "      <td>-6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>nowhiring</td>\n",
       "      <td>-6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17512</th>\n",
       "      <td>blah</td>\n",
       "      <td>-6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29766</th>\n",
       "      <td>plagiarism</td>\n",
       "      <td>-6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36469</th>\n",
       "      <td>first-class</td>\n",
       "      <td>-6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7951</th>\n",
       "      <td>--------</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20899</th>\n",
       "      <td>voicemod</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>journalist</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45203</th>\n",
       "      <td>sunday'</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51649</th>\n",
       "      <td>keh</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55221</th>\n",
       "      <td>real-time</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56824</th>\n",
       "      <td>diaper</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64255</th>\n",
       "      <td>everythingtachaisback</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        word  score  count  ratio\n",
       "53253               maternal    -28     28   -1.0\n",
       "27459            downshifter    -16     16   -1.0\n",
       "88127               paternal    -16     16   -1.0\n",
       "56772             aq7481-600    -14     14   -1.0\n",
       "25976                     5h    -13     13   -1.0\n",
       "67446                   sony     -9      9   -1.0\n",
       "7341                   recep     -7      7   -1.0\n",
       "2013                    esan     -6      6   -1.0\n",
       "5457               nowhiring     -6      6   -1.0\n",
       "17512                   blah     -6      6   -1.0\n",
       "29766             plagiarism     -6      6   -1.0\n",
       "36469            first-class     -6      6   -1.0\n",
       "7951                --------     -5      5   -1.0\n",
       "20899               voicemod     -5      5   -1.0\n",
       "44896             journalist     -5      5   -1.0\n",
       "45203                sunday'     -5      5   -1.0\n",
       "51649                    keh     -5      5   -1.0\n",
       "55221              real-time     -5      5   -1.0\n",
       "56824                 diaper     -5      5   -1.0\n",
       "64255  everythingtachaisback     -5      5   -1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most negative words\n",
    "df.sort_values(by=['ratio', 'count'], ascending=[True, False]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "      <th>count</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24035</th>\n",
       "      <td>iherb</td>\n",
       "      <td>355</td>\n",
       "      <td>355</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25719</th>\n",
       "      <td>aqh3836</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30803</th>\n",
       "      <td>delighted</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49880</th>\n",
       "      <td>slytherin</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30879</th>\n",
       "      <td>heart-shaped</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30936</th>\n",
       "      <td>sun3</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44700</th>\n",
       "      <td>sun1</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31021</th>\n",
       "      <td>nisnass</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44829</th>\n",
       "      <td>vogacloset</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10378</th>\n",
       "      <td>ps4live</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30963</th>\n",
       "      <td>mapc</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15707</th>\n",
       "      <td>offl</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43008</th>\n",
       "      <td>fly18</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46399</th>\n",
       "      <td>go48</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57021</th>\n",
       "      <td>suriya</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5405</th>\n",
       "      <td>wizebot</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24054</th>\n",
       "      <td>banget</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88299</th>\n",
       "      <td>-discount100</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55099</th>\n",
       "      <td>-discount10</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10470</th>\n",
       "      <td>we</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  score  count  ratio\n",
       "24035         iherb    355    355    1.0\n",
       "25719       aqh3836    181    181    1.0\n",
       "30803     delighted     70     70    1.0\n",
       "49880     slytherin     52     52    1.0\n",
       "30879  heart-shaped     51     51    1.0\n",
       "30936          sun3     41     41    1.0\n",
       "44700          sun1     41     41    1.0\n",
       "31021       nisnass     33     33    1.0\n",
       "44829    vogacloset     32     32    1.0\n",
       "10378       ps4live     31     31    1.0\n",
       "30963          mapc     30     30    1.0\n",
       "15707          offl     28     28    1.0\n",
       "43008         fly18     27     27    1.0\n",
       "46399          go48     24     24    1.0\n",
       "57021        suriya     23     23    1.0\n",
       "5405        wizebot     21     21    1.0\n",
       "24054        banget     20     20    1.0\n",
       "88299  -discount100     20     20    1.0\n",
       "55099   -discount10     19     19    1.0\n",
       "10470            we     18     18    1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most positive words\n",
    "df.sort_values(by=['ratio', 'count'], ascending=[False, False]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='score').to_csv('result_word_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_emojis = (\n",
    "    data\n",
    "        .rdd\n",
    "        .filter(lambda tweet: tweet['text'] is not None)\n",
    "        .filter(lambda tweet: no_retweet(tweet['text']))\n",
    "        .filter(lambda tweet: only_emojis(tweet['text']))\n",
    "        .filter(lambda tweet: language_filter(tweet['text'], 'en'))\n",
    "        .map(lambda tweet: {\n",
    "            'sentiment':evaluate_by_words(tweet['text']), \n",
    "            'text':tweet['text']\n",
    "        })\n",
    "        .flatMap(lambda tweet: split_emojis(tweet))\n",
    "        .reduceByKey(add_tuples)\n",
    "        .map(lambda row: (row[0], row[1][0], row[1][1]))\n",
    ")\n",
    "\n",
    "df2 = evaluated_emojis.toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename stuff, create ratio column \n",
    "df2['ratio']=df2.apply(lambda row: row['_2'] / row['_3'], axis=1)\n",
    "df2 = df2.rename(columns={'_1':'word', '_2':'score', '_3':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "      <th>count</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>🕎</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>🛃</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>🚉</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>⏸</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>🈵</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>🐡</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>🚃</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>🚸</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>🛳</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>🍶</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>🦲</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>📴</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>🏩</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>🀄</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>🥥</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>🖇</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>📟</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>🌩</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>💱</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>🦱</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  score  count  ratio\n",
       "1120    🕎     -2      2   -1.0\n",
       "8       🛃     -1      1   -1.0\n",
       "11      🚉     -1      1   -1.0\n",
       "127     ⏸     -1      1   -1.0\n",
       "220     🈵     -1      1   -1.0\n",
       "225     🐡     -1      1   -1.0\n",
       "251     🚃     -1      1   -1.0\n",
       "340     🚸     -1      1   -1.0\n",
       "350     🛳     -1      1   -1.0\n",
       "423     🍶     -1      1   -1.0\n",
       "468     🦲     -1      1   -1.0\n",
       "471     📴     -1      1   -1.0\n",
       "474     🏩     -1      1   -1.0\n",
       "536     🀄     -1      1   -1.0\n",
       "719     🥥     -1      1   -1.0\n",
       "736     🖇     -1      1   -1.0\n",
       "806     📟     -1      1   -1.0\n",
       "854     🌩     -1      1   -1.0\n",
       "934     💱     -1      1   -1.0\n",
       "952     🦱     -1      1   -1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most negative emojis\n",
    "df2.sort_values(by=['ratio', 'count'], ascending=[True, False]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "      <th>count</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>🦌</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>🥐</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>🌘</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>👪</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>🍚</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>🦙</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>🥨</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>🦞</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>🚙</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>📡</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>◾</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>💑</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>🚕</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>🔧</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>🌔</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>🛁</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>🥔</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>📮</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>🦘</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>🚇</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  score  count  ratio\n",
       "122    🦌      5      5    1.0\n",
       "664    🥐      4      4    1.0\n",
       "387    🌘      3      3    1.0\n",
       "524    👪      3      3    1.0\n",
       "54     🍚      2      2    1.0\n",
       "75     🦙      2      2    1.0\n",
       "115    🥨      2      2    1.0\n",
       "148    🦞      2      2    1.0\n",
       "213    🚙      2      2    1.0\n",
       "271    📡      2      2    1.0\n",
       "349    ◾      2      2    1.0\n",
       "416    💑      2      2    1.0\n",
       "442    🚕      2      2    1.0\n",
       "535    🔧      2      2    1.0\n",
       "577    🌔      2      2    1.0\n",
       "628    🛁      2      2    1.0\n",
       "735    🥔      2      2    1.0\n",
       "802    📮      2      2    1.0\n",
       "81     🦘      1      1    1.0\n",
       "124    🚇      1      1    1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most positive emojis\n",
    "df2.sort_values(by=['ratio', 'count'], ascending=[False, False]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values(by='score').to_csv('result_emoji_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(text):\n",
    "    senteval = 0\n",
    "    words = 0\n",
    "    emojis = 0\n",
    "\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    # lemmatize each token\n",
    "    for token in tokenize(text):\n",
    "        words += 1\n",
    "        token = token.lower()\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "\n",
    "        if token in positive_words:\n",
    "            senteval += 1\n",
    "        if token in negative_words:\n",
    "            senteval -= 1\n",
    "\n",
    "    for emoji in get_emojis(text):\n",
    "        emojis+=1\n",
    "        if emoji in df2.values:\n",
    "             senteval += df2.loc[df2['word'] == emoji].values[0][3]\n",
    "                \n",
    "    count = words+emojis\n",
    "    if count == 0:\n",
    "        return 0\n",
    "\n",
    "    return senteval / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_tweets = (\n",
    "    data\n",
    "        .rdd\n",
    "        .filter(lambda tweet: tweet['text'] is not None)\n",
    "        .filter(lambda tweet: no_retweet(tweet['text']))\n",
    "        .filter(lambda tweet: language_filter(tweet['text'], 'en'))\n",
    "        .map(lambda tweet: \n",
    "             (\n",
    "                 evaluate(tweet['text']), tweet['text']\n",
    "             )\n",
    "        )\n",
    ")\n",
    "\n",
    "df3 = evaluated_tweets.toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_1</th>\n",
       "      <th>_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Just praising https://t.co/pTfHVTBPLW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>that was incredible!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Perfectly balanced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Pride of Love https://t.co/Obl4hcL7A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>UNDERSTANDABLE????</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3179</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>If you support me, I will support you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3631</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>right so https://t.co/dGZlviuHX6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Enough https://t.co/ADESDBN5E1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Creative!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I AM IN LOVE https://t.co/LuC1ipKGxW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5848</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>that was so good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5941</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>The best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6581</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>That was cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6628</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Why am I up right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6706</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>the smile :(((:(:(:((( https://t.co/JwhGR9fhxO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>is it faster or is it just me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>How nice to be love and appreciate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>This was beautiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7529</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>how cute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8033</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10048</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>That`s just superb! https://t.co/7DGrS218CM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>that’s tough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11530</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Interesting.  https://t.co/5JJe42JtAm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12120</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>This is good https://t.co/lWn6DX3cms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12536</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>clean https://t.co/u5yhyD2H2u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3903</th>\n",
       "      <td>0.911176</td>\n",
       "      <td>Boom! Succeeded and thriving 🎉</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8406</th>\n",
       "      <td>0.822353</td>\n",
       "      <td>Celebration 🎉 https://t.co/NdIhwfz8nd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5050</th>\n",
       "      <td>0.758881</td>\n",
       "      <td>his smile is so beautiful 🥺https://t.co/WFJAQ1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3418</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>Like and share this pure awesomeness! https://...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             _1                                                 _2\n",
       "735    1.000000              Just praising https://t.co/pTfHVTBPLW\n",
       "750    1.000000                               that was incredible!\n",
       "1390   1.000000                                            Correct\n",
       "1994   1.000000                                Perfectly balanced.\n",
       "2299   1.000000              Pride of Love https://t.co/Obl4hcL7A1\n",
       "2899   1.000000                                 UNDERSTANDABLE????\n",
       "3179   1.000000             If you support me, I will support you.\n",
       "3631   1.000000                   right so https://t.co/dGZlviuHX6\n",
       "4098   1.000000                     Enough https://t.co/ADESDBN5E1\n",
       "4984   1.000000                                          Creative!\n",
       "5426   1.000000               I AM IN LOVE https://t.co/LuC1ipKGxW\n",
       "5848   1.000000                                   that was so good\n",
       "5941   1.000000                                           The best\n",
       "6581   1.000000                                      That was cool\n",
       "6628   1.000000                              Why am I up right now\n",
       "6706   1.000000     the smile :(((:(:(:((( https://t.co/JwhGR9fhxO\n",
       "6988   1.000000                      is it faster or is it just me\n",
       "7210   1.000000                 How nice to be love and appreciate\n",
       "7318   1.000000                                 This was beautiful\n",
       "7529   1.000000                                           how cute\n",
       "8033   1.000000                                              Right\n",
       "10048  1.000000        That`s just superb! https://t.co/7DGrS218CM\n",
       "10501  1.000000                                       that’s tough\n",
       "11530  1.000000              Interesting.  https://t.co/5JJe42JtAm\n",
       "12120  1.000000               This is good https://t.co/lWn6DX3cms\n",
       "12536  1.000000                      clean https://t.co/u5yhyD2H2u\n",
       "3903   0.911176                     Boom! Succeeded and thriving 🎉\n",
       "8406   0.822353              Celebration 🎉 https://t.co/NdIhwfz8nd\n",
       "5050   0.758881  his smile is so beautiful 🥺https://t.co/WFJAQ1...\n",
       "3418   0.750000  Like and share this pure awesomeness! https://..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.nlargest(30, '_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_1</th>\n",
       "      <th>_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>This is how and when I will die https://t.co/V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>HE’S INSANE https://t.co/Xl5E71wd7q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4729</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5240</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>This is bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5387</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>you bitches so hateful it’s disgusting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>Racism is over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5868</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>headache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>Just so wrong.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>What are your symptoms?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6232</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>why is he my bias again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6646</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>Fuck this fuck that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>this is drought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6793</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>I am not a fool.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6967</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>That bitch fell so hard https://t.co/p4r7rE0k21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7556</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>the hell is this https://t.co/MH06R6dS5F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8210</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>*Crying Noises* https://t.co/MNFPir7n3w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8240</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>What is this noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8591</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>what a trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8703</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>i fucking hate it here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>i am exhausted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9145</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>WHAT\\nTHE\\nFUCK https://t.co/VNcBvPu7Dz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9619</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>Blasphemy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9696</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>why did he just scream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10475</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>That’s crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11084</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>Mad irritated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11855</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>I hate it here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12257</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>This shit slowwww</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12748</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>not fucking with it no more.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3358</th>\n",
       "      <td>-0.80</td>\n",
       "      <td>Being sick is fucking bullshit!! Fuck you body!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8502</th>\n",
       "      <td>-0.75</td>\n",
       "      <td>these noises be freaking me the fuck out lmao</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         _1                                                 _2\n",
       "272   -1.00  This is how and when I will die https://t.co/V...\n",
       "1128  -1.00                HE’S INSANE https://t.co/Xl5E71wd7q\n",
       "4729  -1.00                                           Negative\n",
       "5240  -1.00                                        This is bad\n",
       "5387  -1.00            you bitches so hateful it’s disgusting.\n",
       "5620  -1.00                                     Racism is over\n",
       "5868  -1.00                                           headache\n",
       "5903  -1.00                                     Just so wrong.\n",
       "6086  -1.00                            What are your symptoms?\n",
       "6232  -1.00                            why is he my bias again\n",
       "6646  -1.00                                Fuck this fuck that\n",
       "6790  -1.00                                    this is drought\n",
       "6793  -1.00                                   I am not a fool.\n",
       "6967  -1.00    That bitch fell so hard https://t.co/p4r7rE0k21\n",
       "7556  -1.00           the hell is this https://t.co/MH06R6dS5F\n",
       "8210  -1.00            *Crying Noises* https://t.co/MNFPir7n3w\n",
       "8240  -1.00                                 What is this noise\n",
       "8591  -1.00                                       what a trash\n",
       "8703  -1.00                             i fucking hate it here\n",
       "9012  -1.00                                     i am exhausted\n",
       "9145  -1.00            WHAT\\nTHE\\nFUCK https://t.co/VNcBvPu7Dz\n",
       "9619  -1.00                                          Blasphemy\n",
       "9696  -1.00                             why did he just scream\n",
       "10475 -1.00                                       That’s crazy\n",
       "11084 -1.00                                      Mad irritated\n",
       "11855 -1.00                                     I hate it here\n",
       "12257 -1.00                                  This shit slowwww\n",
       "12748 -1.00                       not fucking with it no more.\n",
       "3358  -0.80   Being sick is fucking bullshit!! Fuck you body!!\n",
       "8502  -0.75      these noises be freaking me the fuck out lmao"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.nsmallest(30, '_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
